{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1258ce3-bf4d-41db-b310-a0b61641736c",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3063e6f-8606-4a6b-9e87-f50f21a31f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for file paths\n",
    "import retro # framework to interact with the game\n",
    "from gym import Env # environment base class\n",
    "from gym.spaces import MultiBinary, Box # space shapes for the game environment\n",
    "import numpy as np # for calculating frame changes\n",
    "import cv2 as cv # for image preprocessing\n",
    "import time # to slow down each frame so we can clearly see the game as it's being played\n",
    "import math # to calculate the reward on each step\n",
    "#from matplotlib import pyplot as plt\n",
    "import optuna # for optimizing the agent's hyperparameters easily\n",
    "\n",
    "# For building the DL model\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, InputLayer\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "# For building the DQN agent\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fe36a-efdd-442b-a0f9-e5a2326e51fe",
   "metadata": {},
   "source": [
    "# Game Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea9fa8-6d01-47f5-8c69-3210baafca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying directories where the models will be saved\n",
    "LOG_DIR = \"./logs/\"\n",
    "OPT_DIR = \"./opt/\"\n",
    "TRAIN_DIR = \"./train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b97736-3210-45aa-bc3b-26edae9f9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, A, _, _, UP, DOWN, LEFT, RIGHT, C, Y, X, Z\n",
    "# Every possible action for a given step in Street Fighter II\n",
    "possible_actions = {\n",
    "    # Idle\n",
    "    0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Left\n",
    "    1 : [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    # Right\n",
    "    2 : [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    # Up\n",
    "    3 : [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Down\n",
    "    4 : [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    # Light Kick\n",
    "    5 : [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Medium Kick\n",
    "    6 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    # Hard Kick\n",
    "    7 : [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    # Light Punch\n",
    "    8 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    # Medium Punch\n",
    "    9 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    # Hard Punch\n",
    "    10 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    # Down Left\n",
    "    11 : [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "    # Down Right\n",
    "    12 : [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
    "    # Up Left\n",
    "    13 : [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    # Up Right\n",
    "    14 : [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0c600-bbcf-48ee-90f5-515f078abd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a wrapper to interact with the game environment\n",
    "\n",
    "class StreetFighterWrapper(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(StreetFighterWrapper, self).__init__()\n",
    "        \n",
    "        # Starting health in Street Fighter II is 176 HP\n",
    "        self.START_HEALTH = 176\n",
    "        self.REWARD_COEFF = 17\n",
    "        self.PENALTY_COEFF = 1.75\n",
    "        self.LOSS_PENALTY_COEFF = 0.35\n",
    "        \n",
    "        # defining our observation space to take frames from a\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = (100, 100, 1), dtype=np.uint8)\n",
    "        \n",
    "        # defining our own set of actions that mimics the original\n",
    "        self.action_space = MultiBinary(12)\n",
    "        \n",
    "        # setting up the game environment\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "\n",
    "        # enemy and agent health values that get updated as the game goes along\n",
    "        self.enemy_health = self.START_HEALTH\n",
    "        self.agent_health = self.START_HEALTH\n",
    "\n",
    "        # Creating a score variable to hold the agent's current score; important for calculating the reward on each step\n",
    "        self.score = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Getting the array of button combinations that corresponds to the number generated by the agent\n",
    "        action_arr = possible_actions[action]\n",
    "        \n",
    "        # calculate change in current frame vs previous frame\n",
    "        obs, reward, done, info = self.game.step(action_arr)\n",
    "        obs = self.preprocess(obs)\n",
    "\n",
    "        ## reward = info['score'] - self.score\n",
    "\n",
    "        # calculating the change in health for each player (i.e. health from this frame - health from previous frame)\n",
    "        enemy_damage_taken = abs(info['enemy_health'] - self.enemy_health)\n",
    "        agent_damage_taken = abs(info['health'] - self.agent_health)\n",
    "        \n",
    "        # Tweaking the reward function to be the score of this step - score from the previous step (i.e. the change in score)\n",
    "\n",
    "        # catching edge cases to make sure no reward is being earned outside of a fight (i.e. in between rounds)\n",
    "        if (self.enemy_health != 0 and info['enemy_health'] == 0 and self.agent_health != 0 and info['health'] == 0) or (enemy_damage_taken == 0 and agent_damage_taken == 0) or (self.agent_health == 0 and self.enemy_health == 0):\n",
    "            reward = 0\n",
    "        \n",
    "        # If the agent wins and enemy loses\n",
    "        elif info['enemy_health'] < 0:\n",
    "            reward = self.START_HEALTH * math.log(info['health'], self.START_HEALTH) * self.REWARD_COEFF\n",
    "            \n",
    "        # if the enemy wins and agent loses\n",
    "        elif info['health'] < 0:\n",
    "            reward = -math.pow(self.START_HEALTH, (info['enemy_health'] / self.START_HEALTH)) * self.LOSS_PENALTY_COEFF\n",
    "                               \n",
    "        # the fight goes on\n",
    "        else:\n",
    "            # If the enemy took more damage than the agent\n",
    "            if enemy_damage_taken > agent_damage_taken:\n",
    "                reward = ((enemy_damage_taken) - (agent_damage_taken)) * self.REWARD_COEFF\n",
    "            # If the agent took more or same amount of damage than the enemy\n",
    "            else:\n",
    "                reward = ((enemy_damage_taken) - (agent_damage_taken)) * self.PENALTY_COEFF\n",
    "\n",
    "        # as long as the game hasn't ended, set the \"previous\" health as the current health so we can calculate the reward for the next step\n",
    "        self.enemy_health = info['enemy_health']\n",
    "        self.agent_health = info['health']\n",
    "        \n",
    "        # set the \"previous\" score as the current score so we can calculate the reward for the next step\n",
    "        self.score = info['score']\n",
    "\n",
    "        # only print the reward earned if it's non-zero\n",
    "        if (reward != 0):\n",
    "            print(\"Agent Health: {} Enemy Health: {} Agent Damage Taken: {} Enemy Damage Taken: {} Reward: {}\".format(info['health'], info['enemy_health'], agent_damage_taken, enemy_damage_taken, reward))\n",
    "            ## print(\"Agent Health: {} Enemy Health: {} Reward: {}\".format(info['health'], info['enemy_health'], reward))\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    # Renders the current game frame\n",
    "    def render(self, mode= 'human'):\n",
    "        self.game.render(mode = 'human')\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the observation\n",
    "        obs = self.game.reset()\n",
    "        # Preprocess the observation for the agent\n",
    "        obs = self.preprocess(obs)\n",
    "\n",
    "        # Reset the game data\n",
    "        self.score = 0\n",
    "        self.enemy_health = self.START_HEALTH\n",
    "        self.agent_health = self.START_HEALTH\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, obs):\n",
    "    \n",
    "        # set image to greyscale\n",
    "        greyed = cv.cvtColor(obs, cv.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # and make it smaller\n",
    "        resized = cv.resize(greyed, (100, 100), interpolation=cv.INTER_CUBIC)\n",
    "        \n",
    "        processed = np.reshape(resized, (100, 100, 1))\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def close(self):\n",
    "        # Close the game environment\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98934b1a-6d22-452e-81d0-684a457bf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating our environment and wrapping it in a FrameStack to keep track of the past 4 frames\n",
    "env = StreetFighterWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2f1df-a809-45b7-8561-f08964844a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store observation and action space data\n",
    "height, width, channels = env.observation_space.shape\n",
    "frames = 4\n",
    "actions = len(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41836bb1-8468-49ec-bde0-afc1779d2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Loop\n",
    "\n",
    "# Resets the observations\n",
    "obs = env.reset()\n",
    "\n",
    "# Flag that controls the inner loop\n",
    "done = False\n",
    "\n",
    "# Play one match\n",
    "for game in range(1):\n",
    "    while not done:\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        # renders the game frame\n",
    "        env.render()\n",
    "        # Taking random actions\n",
    "        action = possible_actions[np.random.randint(len(possible_actions))]\n",
    "        # Info contains: continueTimer, enemy_health, enemy_matches_won, health, matches_won, score\n",
    "        obs, reward, done, info = env.step(action) # taking random actions\n",
    "        # To make the game slower so it can be watched more closely\n",
    "        time.sleep(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfee816-cbd0-41ce-923f-5b2ea4c90c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bf96f-80e5-49ac-b301-cbb136b682e9",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "Using the Optuna library to optimize hyperparameters for a Deep Q Network.\n",
    "These hyperparameters include:\n",
    "DL Model: # of layers, # of units in each Dense layer,\n",
    "DQN Agent: SequentialMemory - limit,\n",
    "LinearAnnealedPolicy - value_test,\n",
    "Other agent parameters: target_model_update, learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db690c5a-0e2e-450a-be9d-a84d80407ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(trial):\n",
    "    # Testing either 3 or 4 Dense layers\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 3, 4)\n",
    "\n",
    "    # Creating the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding the input layer - input shape \n",
    "    model.add(Flatten(input_shape = (frames, height, width, channels)))\n",
    "    \n",
    "    # Suggest values of the number of units in each Dense layer\n",
    "    for i in range(n_layers):\n",
    "        model.add(\n",
    "            Dense(units = trial.suggest_categorical(f'n_units_l{i}', [64, 96, 128, 256, 512]),\n",
    "                  activation = 'relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    # Keras RL needs a Flatten layer at the end for the agent to work\n",
    "    model.add(Flatten())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc2119-79a6-4c52-bcc5-97e0451b4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dqn(trial):\n",
    "    # Testing out various hyperparameters for the DQN Agent\n",
    "    dqn_params = {\n",
    "        'limit': trial.suggest_int('limit', 1000, 3000),\n",
    "        'policy': LinearAnnealedPolicy(EpsGreedyQPolicy(), attr = 'eps', value_max = 1.0, value_min = 0.1, value_test = trial.suggest_float('value_test', 0.05, 0.2), nb_steps = 30000),\n",
    "        'target_model_update': trial.suggest_float('target_model_update', 0.01, 1.0, log = True),\n",
    "        'learning_rate' : trial.suggest_float('learning_rate', 1e-7, 1e-3, log = True)\n",
    "    }\n",
    "    \n",
    "    return dqn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa20e9-0c6b-4dc0-b013-6856f23d026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        # Getting the DQN and model parameters\n",
    "        model = optimize_model(trial)\n",
    "        dqn_params = optimize_dqn(trial)\n",
    "    \n",
    "        # Create environment\n",
    "        env = StreetFighterWrapper()\n",
    "        \n",
    "        # Creating the DQN\n",
    "        agent = DQNAgent(\n",
    "        model = model,\n",
    "        memory = SequentialMemory(limit = dqn_params['limit'], window_length = frames), # window length has to align with input shape in the NN\n",
    "        policy = dqn_params['policy'],\n",
    "        enable_dueling_network = True, dueling_type = 'avg',\n",
    "        nb_actions = actions,\n",
    "        nb_steps_warmup = 5000,\n",
    "        target_model_update = dqn_params['target_model_update']\n",
    "        )\n",
    "\n",
    "        # Compile and fit the agent to the environment\n",
    "        agent.compile(Adam(learning_rate = (dqn_params['learning_rate'])), metrics = ['mae'])\n",
    "        agent.fit(env, 30000, action_repetition = 1, callbacks = None, verbose = 2, visualize = False)\n",
    "    \n",
    "        # Evaluate the model\n",
    "        scores = agent.test(env, nb_episodes = 10, visualize = False)\n",
    "        mean_reward = np.mean(scores.history['episode_reward'])\n",
    "        print(f\"Mean reward for trial {trial.number} is {mean_reward}\")\n",
    "        env.close()\n",
    "    \n",
    "        SAVE_PATH = os.path.join(OPT_DIR, f\"trial_{trial.number}_best_agent_weights.h5f\")\n",
    "        agent.save_weights(SAVE_PATH)\n",
    "    \n",
    "        return mean_reward\n",
    "        \n",
    "    except Exception as e:\n",
    "        return -500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369fcf3-40dc-491c-acbc-70edfa335810",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f74bf7-4811-4ed2-b385-eb39e01caf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the experiment to find the optimal DQN and model hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials = 150, n_jobs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68147aa5-e0dc-4f17-a6df-90848550350a",
   "metadata": {},
   "source": [
    "# Building the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e711489-f71d-47c7-84f8-ecefc4eb9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the best agent hyperparameters and the number of the trial that had the best results\n",
    "params = study.best_params\n",
    "best_trial_number = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb493f2-45d0-4d67-b42c-81e0da7afdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighterWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59694c1-2fe8-4e7f-a907-3ee841bade3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params):\n",
    "    # Creating the model with tuned hyperparameters\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = (frames, height, width, channels)))\n",
    "\n",
    "    for i in range(params['n_layers']):\n",
    "        model.add(Dense(units = params[f\"n_units_l{i}\"], activation = 'relu'))\n",
    "\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.add(Flatten()) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bcfca-04bf-4072-8d12-a5d50ff21a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dqn_agent(params):\n",
    "    # Creating the agent and setting hyperparameters to the tuned values\n",
    "    agent = DQNAgent(\n",
    "    model = build_model(params),\n",
    "    memory = SequentialMemory(limit = params['limit'], window_length = frames), # window length has to align with input shape in the NN\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr = 'eps', value_max = 1.0, value_min = 0.1, value_test = params['value_test'], nb_steps = 30000),\n",
    "    enable_dueling_network = True, dueling_type = 'avg',\n",
    "    nb_actions = actions,\n",
    "    nb_steps_warmup = 1000,\n",
    "    target_model_update = params['target_model_update']\n",
    "    )\n",
    "\n",
    "    # Loading the weights from the best trial so it gets a headstart when training\n",
    "    agent.load_weights(f\"{OPT_DIR}/trial_{best_trial_number}_best_agent_weights.h5f\")\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def4653-a955-4dca-a229-46f0be9262d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DQN agent with optimized hyperparameters\n",
    "agent = build_dqn_agent(params)\n",
    "# Compiling the agent\n",
    "agent.compile(Adam(learning_rate = (params['learning_rate'])), loss = 'MeanSquaredError()', metrics = ['mae'])\n",
    "num_steps = 100000\n",
    "# Training the agent\n",
    "agent.fit(env, num_steps, action_repetition = 1, callbacks = None, verbose = 2, visualize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44375bdb-30fd-449a-89c9-a38a1fc13a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_weights(f\"{TRAIN_DIR}/agent_{num_steps}_steps.h5f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a885d-4f1e-4122-a4ee-78ec711472f4",
   "metadata": {},
   "source": [
    "# Testing out the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54260791-7ba2-4105-8990-9dc16332c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_weights(f\"{TRAIN_DIR}/agent_{num_steps}_steps.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43587736-7883-4a37-8da9-9d801096ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the observations\n",
    "obs = env.reset()\n",
    "\n",
    "# Flag that controls the inner loop\n",
    "done = False\n",
    "\n",
    "# Play one match\n",
    "for game in range(1):\n",
    "    while not done:\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        # renders the game frame\n",
    "        env.render()\n",
    "        action = agent.forward(obs)\n",
    "        # Info contains: continueTimer, enemy_health, enemy_matches_won, health, matches_won, score\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
